{
  // example quiz text
  // <- (this is a comment and will be ignored)

  // this is the url for your quiz
  "url": "https://github.com/jb9474/ml_quiz",

  // this are your UoB candidate numbers as a comma separated list
  "candidate_number": [25035],

  // this is the title of the quiz
  "title": "Jeremy Barker Quiz",


  "1": {
    "difficulty": "3",
    "reference": "1.2",
    "problem_type": "problem solving",
    "question": "A first year student enters a maths class looking slightly unwell. Given the assumptions below, what are the odds that this student has been on a night out? Given that, 95% chance if they have been out, they will look ill, given that any student diet isn't fantastic if they haven't been out 30% chance they look unwell anyway, and that, as the student is a fresher, 40% chance they will have been out!",
    "answer_type": "multiple",
    "answers": [
      {
        "correctness": "+",
        "answer": "2.1: this student has been drinking!",
        "explanation": "use equation for posterior odds"
      },
      {
        "correctness": "-",
        "answer": "0.9: Just freshers flu"
      }],
    "hint": "should be straightforward..",
    "workings": "p(look unwell | on night out) = 0.95, p(look unwell | not been on night out) = 0.3, p(been on night out) = 0.4. Odds(been out | look ill) = L (look ill | been out) x O (been out) = (0.95/0.3) x (0.4 / 1- 0.4) = 2.1 > 1 so numerator most likely.",
    "source": "Probabilistic Reasoning in Intelligent Systems, Judea Pearl",
    "comments": "careful to use Odds as ratios."
  },

  "2": {
    "difficulty": "2",
    "reference": "6.2",
    "problem_type": "calculation",
    "question": "Calculate the ranking error of the following rule list:<br>\
        if length = 8, class = \u2295 [2+, 5-]<br>\
        else if teeth = yes class = \u2296 [6+, 3-]<br>\
        else class = \u2296 [0+, 1-]",
    "answer_type": "multiple",
    "answers": [
      {
        "correctness": "+",
        "answer": "20",
        "explanation": "n/a"
      },
      {
        "correctness": "-",
        "answer": "14"
      },
      {
        "correctness": "-",
        "answer": "29"
      }],
    "hint": "consider half errors and full errors",
    "workings": "10x1/2 + 18x1/2 + 6x1 + 3x0 = 20",
    "source": "Flach, Machine Learning",
    "comments": "careful to use Odds as ratios."
  },

  "3": {
    "difficulty": "5",
    "reference": "0",
    "problem_type": "beyond scope of book",
    "answer_type": "blank_answer",
    "question": [
      " \"A two-by-two contingency table with relative frequencies has", 1, "degrees of freedom. A typical choice related to ROC analysis is to use the false positive rate fpr=FP/NEG on the X-axis, true positive rate tpr=TP/POS on the Y-axis, and", 3, "on the Z-axis. This is 3D ROC space. The key assumption of ROC analysis is that true and false positive rates describe the performance of the model", 2, "of the class distribution, and we are thus free to manipulate the Z-axis in 3D ROC space to conform to the class distribution of the environment in which the model will be employed.\" "],
      "answers": [
      { "correctness": 1,
        "answer": "3",
        "explanation": "for each set of false positive and true positive rates the class distribution will affect the classifiers accuracy."
      },
      { "correctness": 2,
        "answer": "independently",
        "explanation": "normalisation factors away this independence in a normalised coverage ROC plot"
      },
        { "correctness": 3,
        "answer": "the relative frequency of positives pos=POS/(POS+NEG)",
        "explanation": "in this 3D ROC space the user is now free to conform to the class distribution of the environment in which the model will be employed."
      }
    ],
    "hint": "a ROC plot allows the comparison of different classifiers irrespective of their respective class distributions",
    "workings": "n/a",
    "source": "Flach, P.A (2003) The geometry of ROC space: understanding machine learning metrics through ROC isometrics. In T. Fawcett and N. Mishra (eds.) Proceedings of the Twentieth International Conference on Machine Learning (ICML 2003), pp. 194-201. AAAI Press. 156",
    "comments": "Expanding knowledge from 2d ROC space to 3D for more sophisticated model analysis"
  },

  "4": {
    "difficulty": "1",
    "reference": "3.3",
    "problem_type": "definition",
    "question": "Match the following sentences.",
    "answer_type": "matrix_sort_answer",
    "answers": [
      { "correctness": "\"model can be applied to new data\"",
        "answer": "'predictive'.",
        "explanation": "model has been learned."
      },
      { "correctness": "\"model only relevant to data given\"",
        "answer": "'descriptive'.",
        "explanation": "n/a"
      },
      { "correctness": "\"model trained on labelled data\"",
        "answer": "'supervised learning'.",
        "explanation": "n/a"
      },
      { "correctness": "\"model trained on unlabelled data\"",
        "answer": "'unsupervised learning'.",
        "explanation": "n/a"
      }
    ],
    "hint": "you do the alphabet",
    "workings": "how to arrive at the correct answer and why all the others are incorrect",
    "source": "some book",
    "comments": "What's special about this question?"
  },

  "5": {
    "difficulty": "2",
    "reference": "5.1",
    "problem_type": "calculation",
    "question": "Time machines now exist and Bob and Jane go back in time to classify some T-rexs. \
        Using the labels and information below, but ignoring the t-rex column, place \
        the features in ascending order of total entropy and using this measure, thus show which would be the best split \
        at the root of a decision tree.<br>\
        <table>\
        <tr>\
          <th><b>Length</b></th>\
          <th><b>Claws</b></th>\
          <th><b>Teeth</b></th>\
          <th><b>Long Tail</b></th>\
          <th><b>T-Rex</b></th>\
        </tr>\
        <tr>\
          <td>9</td>\
          <td>yes</td>\
          <td>yes</td>\
          <td>no</td>\
          <td>yes</td>\
        </tr>\
        <tr>\
          <td>10</td>\
          <td>yes</td>\
          <td>yes</td>\
          <td>yes</td>\
          <td>yes</td>\
        </tr>\
        <tr>\
          <td>9</td>\
          <td>yes</td>\
          <td>yes</td>\
          <td>yes</td>\
          <td>yes</td>\
        </tr>\
        <tr>\
          <td>8</td>\
          <td>no</td>\
          <td>no</td>\
          <td>yes</td>\
          <td>no</td>\
        </tr>\
        <tr>\
         <td>8</td>\
          <td>yes</td>\
          <td>yes</td>\
          <td>no</td>\
          <td>no</td>\
        </tr>\
        <tr>\
         <td>9</td>\
          <td>no</td>\
          <td>yes</td>\
          <td>yes</td>\
          <td>no</td>\
        </tr>\
      </table>",
    "answer_type": "sort",
    "answers": [
      { "correctness": "4",
        "answer": "long tail",
        "explanation": "entropy 1"
      },
      { "correctness": "1",
        "answer": "length",
        "explanation": "entropy 0.46"
      },
      { "correctness": "3",
        "answer": "teeth",
        "explanation": "0.8"
      },
      { "correctness": "2",
        "answer": "claws",
        "explanation": "0.54"
      }
    ],
    "hint": "use the weighted average impurity. From inspection can see those features with entropy 1 and 0.",
    "workings": "length[8,9,10]->[0+,2-][2+,1-][1+,0-]\
      claws[yes,no]->[3+,1-][0+,2-]\
      teeth[yes,no]->[3+,2-][0+,1-]\
      long-tail[yes,no]->[2+,2-][1+,1-]\
      calc:\
      length (6/2*0)+(3/6*0.92)+(1/6*0)=0.46\
      claws (4/6*0.81)+(2/7*0)=0.54\
      teeth (5/6*0.97)+(3/6*0)=0.8\
      long-tail (4/6*1)+(2/6*1)=1",
    "source": "Machine Learning, Flach.",
    "comments": "straightforward calculations."
  },

  "5": {
    "difficulty": "4",
    "reference": "3.3",
    "problem_type": "beyond scope of the book",
    "answer_type": "blank_answer",
    "question": [
      "$2+2$ is ", 1, ". ",
      "What colour is a red truck: ", 2, "."
    ],
    "answers": [
      { "correctness": 1,
        "answer": "4",
        "explanation": "why this answer is correct"
      },
      { "correctness": 2,
        "answer": "red",
        "explanation": "why this answer is correct"
      }
    ],
    "hint": "you do the alphabet",
    "workings": "how to arrive at the correct answer and why all the others are incorrect",
    "source": "some book",
    "comments": "What's special about this question?"
  },

  "6": {
    "difficulty": "3",
    "reference": "3.3",
    "problem_type": "beyond scope of the book",
    "question": "Fill in contingency matrix.<br><br>1TP<br>5TN<br>12 in total<br>5 actual positive",
    "answer_type": "cloze_answer",
    "answers": {
      "answer": ["1 | 4 | 5 ",
                 "----------",
                 "2 | 5 | 7 ",
                 "----------",
                 "3 | 9 | 12"],
      "explanation": "why this answer is correct"
    },
    "hint": "you do the alphabet",
    "workings": "how to arrive at the correct answer and why all the others are incorrect",
    "source": "some book",
    "comments": "What's special about this question?"
  },

  "7": {
    "difficulty": "5",
    "reference": "3.3",
    "problem_type": "beyond scope of the book",
    "question": "Match the following sentences.",
    "answer_type": "matrix_sort_answer",
    "answers": [
      { "correctness": "The letter after 'a' is ",
        "answer": "'B'.",
        "explanation": "why this answer is correct"
      },
      { "correctness": "The letter after 'g' is ",
        "answer": "'H'.",
        "explanation": "why this answer is correct"
      },
      { "correctness": "I like ",
        "answer": "alphabet.",
        "explanation": "why this answer is correct"
      }
    ],
    "hint": "you do the alphabet",
    "workings": "how to arrive at the correct answer and why all the others are incorrect",
    "source": "some book",
    "comments": "What's special about this question?"
  }
}
